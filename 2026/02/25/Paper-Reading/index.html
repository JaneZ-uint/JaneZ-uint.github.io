<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Paper Reading 1 | Brain of JaneZ in a Jar</title><meta name="author" content="JaneZ"><meta name="copyright" content="JaneZ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Paper Reading RecordANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization （MICRO 2022）核心目标：在保持模型精度的前提下，把 DNN 量化做到 4-bit，同时硬件代价极小。 量化的想法：把“高精度浮点数”变成“低精度整数” 比如深度神经网络里">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Reading 1">
<meta property="og:url" content="https://janezair.site/2026/02/25/Paper-Reading/index.html">
<meta property="og:site_name" content="Brain of JaneZ in a Jar">
<meta property="og:description" content="Paper Reading RecordANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization （MICRO 2022）核心目标：在保持模型精度的前提下，把 DNN 量化做到 4-bit，同时硬件代价极小。 量化的想法：把“高精度浮点数”变成“低精度整数” 比如深度神经网络里">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://janezair.site/img/Judy.jpg">
<meta property="article:published_time" content="2026-02-25T08:09:56.000Z">
<meta property="article:modified_time" content="2026-02-26T05:27:05.498Z">
<meta property="article:author" content="JaneZ">
<meta property="article:tag" content="Compiler">
<meta property="article:tag" content="mlsys">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://janezair.site/img/Judy.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Paper Reading 1",
  "url": "https://janezair.site/2026/02/25/Paper-Reading/",
  "image": "https://janezair.site/img/Judy.jpg",
  "datePublished": "2026-02-25T08:09:56.000Z",
  "dateModified": "2026-02-26T05:27:05.498Z",
  "author": [
    {
      "@type": "Person",
      "name": "JaneZ",
      "url": "https://janezair.site"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://janezair.site/2026/02/25/Paper-Reading/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4-b1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Paper Reading 1',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/HK.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Brain of JaneZ in a Jar</span></a><a class="nav-page-title" href="/"><span class="site-name">Paper Reading 1</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Paper Reading 1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2026-02-25T08:09:56.000Z" title="Created 2026-02-25 16:09:56">2026-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-26T05:27:05.498Z" title="Updated 2026-02-26 13:27:05">2026-02-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Paper-Reading-Record"><a href="#Paper-Reading-Record" class="headerlink" title="Paper Reading Record"></a>Paper Reading Record</h1><h2 id="ANT-Exploiting-Adaptive-Numerical-Data-Type-for-Low-bit-Deep-Neural-Network-Quantization-（MICRO-2022）"><a href="#ANT-Exploiting-Adaptive-Numerical-Data-Type-for-Low-bit-Deep-Neural-Network-Quantization-（MICRO-2022）" class="headerlink" title="ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization （MICRO 2022）"></a>ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization （MICRO 2022）</h2><p>核心目标：在保持模型精度的前提下，把 DNN 量化做到 4-bit，同时硬件代价极小。</p>
<p><strong>量化</strong>的想法：把“高精度浮点数”变成“低精度整数”</p>
<p>比如深度神经网络里有：权重（weights）、激活值（activations），这些默认都是：FP32（32位浮点数）。问题是：占内存大、计算慢、功耗高、带宽压力大。</p>
<p>ANT 用“固定长度的自适应数值类型”同时利用张量间和张量内的分布差异，在几乎零硬件代价下实现 4-bit 量化。</p>
<h2 id="OliVe-Accelerating-Large-Language-Models-via-Hardware-friendly-Outlier-Victim-Pair-Quantization-（ISCA-2023）"><a href="#OliVe-Accelerating-Large-Language-Models-via-Hardware-friendly-Outlier-Victim-Pair-Quantization-（ISCA-2023）" class="headerlink" title="OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization （ISCA 2023）"></a>OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization （ISCA 2023）</h2><p>核心目标：如何在保证精度的情况下，把大模型（LLM）量化到 4-bit，并且还能高效地在硬件上运行？</p>
<p>量化是最有效的降本方法。把 FP32 &#x2F; FP16 换成低精度（如 int8 &#x2F; int4）。但Transformer 的量化非常难，尤其是权重矩阵中的 outliers（极端值）。这些 outliers 可能占很小比例，但对模型性能影响很大。Transformer 的异常值远大于 CNN。</p>
<p>本文的核心思想是Outlier-Victim Pair（OVP）。让 normal value “牺牲”，给 outlier 腾位置。把“稀疏编码思路”换成“局部牺牲邻居”的思路。</p>
<h2 id="M-ANT-Efficient-Low-bit-Group-Quantization-for-LLMs-via-Mathematically-Adaptive-Numerical-Type-（HPCA-2025）"><a href="#M-ANT-Efficient-Low-bit-Group-Quantization-for-LLMs-via-Mathematically-Adaptive-Numerical-Type-（HPCA-2025）" class="headerlink" title="M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type （HPCA 2025）"></a>M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type （HPCA 2025）</h2><p>核心目标：如何在 大模型（LLM）低比特 group 量化 下，同时做到</p>
<ul>
<li>✅ 高精度</li>
<li>✅ 高计算效率</li>
<li>✅ 支持 KV cache 实时量化</li>
<li>✅ 硬件友好</li>
</ul>
<p>已有方法的问题：Group-wise quantization 成为主流，但文章中发现group 内部分布差异非常剧烈。</p>
<p>核心思想：Mathematically Adaptive Numerical Type。不再“选数据类型”，而是用一个数学公式生成“无限可调的数据类型”。</p>
<p>KV cache 的创新：由于V cache 是时间上逐 token 生成，一个 group 要多次 iteration 才填满，提出 temporal real-time quantization，核心思想：边生成边更新 scale、用 streaming 方式维护最大值、不等 group 完整，这是他们一个硬件级创新。</p>
<h2 id="VQ-LLM-High-performance-Code-Generation-for-Vector-Quantization-Augmented-LLM-Inference-（HPCA-2025）"><a href="#VQ-LLM-High-performance-Code-Generation-for-Vector-Quantization-Augmented-LLM-Inference-（HPCA-2025）" class="headerlink" title="VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference （HPCA 2025）"></a>VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference （HPCA 2025）</h2><p>留个坑，有空看（</p>
<h2 id="ClusterFusion-Expanding-Operator-Fusion-Scope-for-LLM-Inference-via-Cluster-Level-Collective-Primitive-（Nips-2025）"><a href="#ClusterFusion-Expanding-Operator-Fusion-Scope-for-LLM-Inference-via-Cluster-Level-Collective-Primitive-（Nips-2025）" class="headerlink" title="ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive （Nips 2025）"></a>ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive （Nips 2025）</h2><p>LLM Decode：每生成一个 token，都要：</p>
<ul>
<li>把之前所有 token + 新 token 输入模型</li>
<li>重新跑一遍 Transformer block</li>
<li>输出下一个 token 概率</li>
<li>采样</li>
<li>再重复</li>
</ul>
<p><strong>在大模型推理中，95%以上的延迟来自 decoding 阶段</strong>，每生成一个 token 都要跑一整套 Transformer block。</p>
<p>在 NVIDIA Hopper 架构（H100）上，引入了：Thread Block Cluster，Distributed Shared Memory (DSMEM)</p>
<p>我们知道A100 CPU，CUDA执行层次为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Grid</span><br><span class="line"> └── Block</span><br><span class="line">      └── Thread</span><br></pre></td></tr></table></figure>

<p>Hopper 引入：Thread Block Cluster，结构变成：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Grid</span><br><span class="line"> └── Cluster   ← 新增层级</span><br><span class="line">      └── Block</span><br><span class="line">           └── Thread</span><br></pre></td></tr></table></figure>

<p>也就是说：</p>
<p>以前：</p>
<ul>
<li>每个 Block 只能用自己的 shared memory</li>
<li>想跨 block 通信 → 只能用 global memory（慢）</li>
</ul>
<p>现在：</p>
<ul>
<li>一组 Block 可以被调度到一组邻近 SM 上</li>
<li>它们可以共享更快的内存</li>
<li>可以同步</li>
</ul>
<p><strong>核心创新：提出两个 cluster-level primitive</strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://janezair.site">JaneZ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://janezair.site/2026/02/25/Paper-Reading/">https://janezair.site/2026/02/25/Paper-Reading/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Compiler/">Compiler</a><a class="post-meta__tags" href="/tags/mlsys/">mlsys</a></div><div class="post-share"><div class="social-share" data-image="/img/Judy.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2026/01/30/xv6-Learning/" title="xv6 Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">xv6 Learning</div></div><div class="info-2"><div class="info-item-1">一份我学习 mit 6.s081 操作系统课程时的笔记，gitbook链接 xv6 Learning操作系统接口操作系统的四大职能 资源共享：在多个程序间分配计算机资源。  硬件抽象：提供比底层硬件更易用的服务（如 Word 无需关心硬盘型号）。  多路复用：让多个程序看起来在同时运行。  受控交互：管理程序间的数据共享与协作。   我的理解是一种“中介”角色，操作系统在硬件和应用程序之间提供一个抽象层，使得应用程序可以更方便地使用硬件资源，而不需要直接与硬件打交道。 内核与系统调用 内核 (Kernel)：向运行程序提供服务的特殊程序。  进程 (Process)：运行中的程序，拥有独立的指令、数据和栈空间。  硬件保护机制：内核拥有硬件特权，而用户进程没有。  系统调用 (System Call)：  用户进程请求内核服务的唯一接口。  执行时硬件特权级提升，进入内核，完成后返回用户空间。     shell   ——system call——&gt; kernel 进程总是在用户空间和内核空间之间交替运行。 核心机制详解 A. 进程管理 (Process &amp; Mem...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/01/25/Explore-RCompiler/" title="Explore RCompiler"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-25</div><div class="info-item-2">Explore RCompiler</div></div><div class="info-2"><div class="info-item-1">埋一个坑，有空整理一下自己在完成RCompiler和学习编译原理过程中学到的东西。况且下学期也选了高编，也可以往里面加后端和优化的内容。 CodegenRCompiler的Codegen部分主要是将LLVM IR转换为目标平台的机器码。这个过程涉及到指令选择、寄存器分配和指令调度等多个步骤。目前我正在实现最naive的codegen，也就是不涉及寄存器分配和指令调度的版本，把所有虚拟寄存器全部spill到栈上，直接将LLVM IR中的每条指令翻译成对应的机器指令。 指令选择指令选择是将LLVM IR中的每条指令翻译成对应的机器指令的过程。由于LLVM IR是一种中间表示，它的指令集相对较小，而目标平台的指令集可能非常复杂，因此需要一个映射关系来将LLVM IR中的指令翻译成目标平台的机器指令。 </div></div></div></a><a class="pagination-related" href="/2026/01/29/Mlsys-Learning/" title="Mlsys Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-29</div><div class="info-item-2">Mlsys Learning</div></div><div class="info-2"><div class="info-item-1">挖个坑，记录一下寒假读的一些paper和入门mlsys的过程。 关于os可见 xv6-learning，记录了我通过mit教学用的xv6操作系统学习操作系统的过程。 LLM LearningRAG理论提出在2020年之前，学术界已有将检索与生成结合的零星尝试，但尚未形成系统化的方法论。2020年，这一领域迎来了两个具有里程碑意义的工作：Facebook在论文Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks正式提出”RAG”概念并将其应用于知识密集型任务，同时Google的REALM通过在预训练阶段融入潜在知识检索器，显著提升了开放域问答的性能。 ChatGPT发布后，RAG研究更是进入了加速发展的黄金时期。在这个过程中，RAG技术逐渐从单一的检索-生成框架，演化为包含多跳推理、记忆增强和多模态等复杂功能的综合系统。 Naive RAG2020年10月，Meta团队在论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》中...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Judy.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">JaneZ</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JaneZ-uint"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to explore the Brain of JaneZ in a Jar!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Paper-Reading-Record"><span class="toc-number">1.</span> <span class="toc-text">Paper Reading Record</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ANT-Exploiting-Adaptive-Numerical-Data-Type-for-Low-bit-Deep-Neural-Network-Quantization-%EF%BC%88MICRO-2022%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization （MICRO 2022）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OliVe-Accelerating-Large-Language-Models-via-Hardware-friendly-Outlier-Victim-Pair-Quantization-%EF%BC%88ISCA-2023%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization （ISCA 2023）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#M-ANT-Efficient-Low-bit-Group-Quantization-for-LLMs-via-Mathematically-Adaptive-Numerical-Type-%EF%BC%88HPCA-2025%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type （HPCA 2025）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VQ-LLM-High-performance-Code-Generation-for-Vector-Quantization-Augmented-LLM-Inference-%EF%BC%88HPCA-2025%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference （HPCA 2025）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ClusterFusion-Expanding-Operator-Fusion-Scope-for-LLM-Inference-via-Cluster-Level-Collective-Primitive-%EF%BC%88Nips-2025%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive （Nips 2025）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/25/Paper-Reading/" title="Paper Reading 1">Paper Reading 1</a><time datetime="2026-02-25T08:09:56.000Z" title="Created 2026-02-25 16:09:56">2026-02-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/30/xv6-Learning/" title="xv6 Learning">xv6 Learning</a><time datetime="2026-01-30T13:50:51.000Z" title="Created 2026-01-30 21:50:51">2026-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/29/26-Winter/" title="26 Winter">26 Winter</a><time datetime="2026-01-29T13:02:53.000Z" title="Created 2026-01-29 21:02:53">2026-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/29/Mlsys-Learning/" title="Mlsys Learning">Mlsys Learning</a><time datetime="2026-01-29T12:37:11.000Z" title="Created 2026-01-29 20:37:11">2026-01-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/25/Explore-RCompiler/" title="Explore RCompiler">Explore RCompiler</a><time datetime="2026-01-24T17:58:19.000Z" title="Created 2026-01-25 01:58:19">2026-01-25</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By JaneZ</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4-b1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4-b1"></script><script src="/js/main.js?v=5.5.4-b1"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>